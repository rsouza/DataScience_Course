{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Loading libraries and Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas, numpy, seaborn and matplotlib libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import train_test_split to separate train and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import MissingIndicator and SimpleImputer from impute module\n",
    "from sklearn.impute import MissingIndicator\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the parameters and the style for plotting\n",
    "params = {'figure.figsize':(12,8),\n",
    "         'axes.labelsize':13,\n",
    "         'axes.titlesize':16,\n",
    "         'xtick.labelsize':11,\n",
    "         'ytick.labelsize':11\n",
    "         }\n",
    "plt.rcParams.update(params)\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be using Titanic dataset to explore the missing data in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset 'Data/titanic_data.csv' and store it in variable data\n",
    "data = pd.read_csv('../Data/titanic_data.csv')\n",
    "# Get the first 5 rows\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. First look at the missing values\n",
    "\n",
    "We can use Pandas chained `isnull().sum()` function to detect missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the total number of missing values using\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that three columns contain missing values: 'Age', 'Cabin' and 'Embarked'. If we want to compute the proportion of missing values, we can use `.mean()` function and plot the proportion using a barplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the proportion of missing values\n",
    "percentage = data.isnull().mean()*100\n",
    "percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot barchart\n",
    "percentage.plot(kind='bar');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to visualize the location of missing values, we can use seaborn's `heatmap` that tells us where the missing values occur. We set parameter `cbar = False` as the color bar does not need to be drawn.\n",
    "\n",
    "Such a visualization has a benefit which people usually do not realize: Imagine that you just produce sums or in other words amounts of missing values in the dataset. Remember that descriptive statistics might reveal less than what visualisation does. This is also true for missing values. You might be able to spot, for example, **that missing values in two columns have a similar or the same pattern**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize only those three columns that contain missing values\n",
    "data_copy = data[['Age','Cabin','Embarked']]\n",
    "sns.heatmap(data_copy.isnull(), cbar = False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For even better visualization of missing values, we can again use the dedicated library [missingno](https://github.com/ResidentMario/missingno)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import missingno as msno\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "msno.heatmap(data, ax=ax);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "msno.dendrogram(data, ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Concepts of missing values\n",
    "\n",
    "According to Rubin's theory $^{1}$, every data point has some probability of being missing in the dataset. The process that governs these probabilities is called **the missing data mechanism**. \n",
    "\n",
    "## 2.1 MNAR: Missing data Not At Random\n",
    "\n",
    "MNAR means that the probability of being missing varies for reasons that are unknown to us. Let's look at the columns 'Age' and 'Cabin'. We found out that the column 'Cabin' contains approximately 77% missing values, the column 'Age' almost 20% missing values. \n",
    "\n",
    "The age or cabin could not be established for people who did not survive that night. We assume that survivors were asked for such information. But can we infer this when we look at the data? In this case, we expect that observations of people who did not survive should have more missing values. Let's find out.\n",
    "\n",
    "*Note: Below is a cool functionality of pandas. The method is called query and allows you to really simply subset your data. Of course you could also solve with the traditional functionality which you already learned, I just wanted to make use of the opportunity.*\n",
    "\n",
    "### 2.1.1 Diagnosing Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the dataset based on people who survived\n",
    "survived = data.query('Survived == 1')\n",
    "survived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the percentage of missing values in the column 'Cabin' for people who survived\n",
    "print('The percentage of missing values: {0:.1f} %'.format(survived['Cabin'].isna().mean()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the dataset based on people who did not survived\n",
    "not_survived = data.query('Survived == 0')\n",
    "not_survived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the percentage of missing values in the column 'Cabin' for people who did not survive\n",
    "print('The percentage of missing values: {0:.1f} %'.format(not_survived['Cabin'].isna().mean()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output we obtained is the same as we expected. There are more missing values (approximately 87.6%) for people who did not survive compared to those that did (60.2 %)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 1 >>>> Now it's your turn to explore the column 'Age' in the same way \n",
    "#             and think about whether the values are missing at random or not\n",
    "\n",
    "print(survived['Age'].isna().mean()*100)\n",
    "print(not_survived['Age'].isna().mean()*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous notebook we've filled in missing values using Pandas `fillna()` method. We can specify a scalar value method such as backward fill ('bfill'), or forward fill ('ffill'), or statistic such as mean, median, or mode of the particular column within this method. If we want to replace missing data with 'bfill' method or 'ffill' method and the previous or the next value is not present, the missing values remain present after the imputation. Also, be careful when filling in missing values with the mean if your data has outliers since the mean is affected by them.\n",
    "\n",
    "This approach of filling missing values should be sufficient if you will use the dataset for simple analysis. However, remember what we discussed: As soon as we want to build a robust pipeline, for example for Machine Learning, we need to be able to save the state. This means that the Pandas functionality may not be the best one. We would need to be manually saving the state of *\"mean which should be imputed\"* somewhere. \n",
    "\n",
    "Luckily, scikit-learn offers a handy alternative in forms of **missing indicator** and **simple imputer**. Both of these are saving the state so that we can easily make those part of our robust pipeline. Let's now take a look at these two.\n",
    "\n",
    "\n",
    "------\n",
    "\n",
    "**Simple Imputer and Missing Indicator**\n",
    "\n",
    "`scikit learn` offers transformers for univariate and multivariate imputation of missing values. You can read more in the [documentation](https://scikit-learn.org/stable/modules/impute.html). Now we demonstrate the usability of the `SimpleImputer()` class from the impute module. You can specify several parameters, such as the placeholder (`np.nan`) for missing values, the imputation strategy, or the value used to replace missing values. Find more [here](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html).\n",
    "\n",
    "\n",
    "\n",
    "**Before we impute the missing values**, it is useful to mark missing values to preserve the information about which values had been missing. We can use `MissingIndicator`, which transforms the dataset into binary variables indicating the presence of missing values (these binary variables will be added to the original training set). See the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.impute.MissingIndicator.html#sklearn.impute.MissingIndicator) for `MissingIndicator`.\n",
    "\n",
    "In essence, the idea behind `MissingIndicator` is that we **preserve extra information** for our model which is if the value was missing. We are hoping that the model might pick up a pattern herein which we missed.\n",
    "\n",
    "Let's split our data into training and testing set, mark missing values, and fill in those using `SimpleImputer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[['Pclass', 'Name', 'Sex', 'Age', 'SibSp',\n",
    "                                                        'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']], \n",
    "                                                    data['Survived'],\n",
    "                                                    test_size = 0.3,\n",
    "                                                    random_state = 42) \n",
    "# Get the shape \n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of missing values\n",
    "X_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Missing Indicator\n",
    "We'll use `MissingIndicator` to mark missing values by setting the parameter `features = 'missing-only'` (by default). If we want to create binary variables for all features, we set the parameter to `features = 'all'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create MissingIndicator object\n",
    "missing_indicator = MissingIndicator(features = 'missing-only')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit X_train with missing_indicator \n",
    "missing_indicator.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the features with missing values\n",
    "missing_indicator.features_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `.features_` attribute we get feature names or the indices of features containing missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the features names\n",
    "X_train.columns[missing_indicator.features_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the transformation of `X_train` returns a boolean, we create a new variable to store the output. After that, we concatenate it to the original `X_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform X_train using missing_indicator and store the output to X_missing\n",
    "X_train_missing = missing_indicator.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Display the output\n",
    "X_train_missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we concatenate the `X_train_missing` boolean variables to the original `X_train`. To distinguish which boolean variable belongs to the original feature in `X_train`, we create new names (otherwise, boolean variables names will be labeled as 0,1 and 2). `X_train_missing` array needs to be converted using `pd.DataFrame`, since only Series and DataFrame objects are valid within the `concat()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new column names for boolean variables\n",
    "# Create an empty list to store the new names\n",
    "indicator_name = []\n",
    "# Iterate over the features with missing values\n",
    "for column in X_train.columns[missing_indicator.features_]:\n",
    "    column_name = column + '_Missing'\n",
    "    # Append new names to the indicator_name list\n",
    "    indicator_name.append(column_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the original X_train and X_train_missing along the columns\n",
    "# Reset the index in X_train and convert X_train_missing to a Pandas DataFrame \n",
    "# Assign new column names stored in indicator_name to the columns parameter\n",
    "X_train = pd.concat([X_train.reset_index(), pd.DataFrame(X_train_missing, columns = indicator_name)], axis = 1)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 2 >>>>> Repeat the process for X_test \n",
    "# Transform X_test data using missing_indicator and store it to the variable X_test_missing\n",
    "\n",
    "X_test_missing = missing_indicator.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 2 >>>>> Concatenate the original X_test and X_test_missing along the columns in the same way as we did for X_train\n",
    "# Assign it to the original X_test\n",
    "\n",
    "X_test = pd.concat([X_test.reset_index(), pd.DataFrame(X_test_missing, columns = indicator_name)], axis = 1)\n",
    "\n",
    "# Display X_test to see the result\n",
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 Simple Imputer\n",
    "Now we'll impute the missing values of the column 'Age' using `SimpleImputer`. We specify the placeholder for missing values (`np.nan`) and `strategy = 'mean'` (this strategy is by default, so it is okay if you don't explicitly specify it within the class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SimpleImputer object for imputing missing values with mean strategy\n",
    "imputer = SimpleImputer(missing_values = np.nan, strategy = 'mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit column 'Age' in X_train\n",
    "imputer.fit(X_train[['Age']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To see the mean value that will be used for imputing we can use .statistics_ attribute\n",
    "imputer.statistics_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform column 'Age' in X_train\n",
    "X_train['Age'] = imputer.transform(X_train[['Age']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the total number of missing values in column 'Age' to see whether these values have been replaced\n",
    "X_train['Age'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 3 >>>>> Repeat the imputing also for column 'Age' in X_test data\n",
    "X_test['Age'] = imputer.transform(X_test[['Age']])\n",
    "\n",
    "# # Get the total number of missing values in column Age to see whether these values have been replaced\n",
    "X_train['Age'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 MCAR: Missing data Completely At Random \n",
    "\n",
    "When data is missing completely at random, the probability of being missing is the same for all observations in the dataset, i.e. the cause of the missing data is unrelated to the data.\n",
    "\n",
    "Let's take as an example the column 'Embarked' and its missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the rows where the values in the 'Embarked' column are missing\n",
    "data[data['Embarked'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mrs. Stone was traveling in the first class with her maid Miss. Amelie Icard. They occupied the same Cabin B28, but the data on the port of embarkation is missing. But we cannot tell if the 'Embarked' variable depends on any other variable. We can also see that these women have survived, so we assume that they were asked for that information. It could happen that this information was lost when the dataset was created. The probability of losing this information is the same for every person on the Titanic. However, this would probably be impossible to prove. \n",
    "\n",
    "For curiosity: You can find out more information about Mrs. Stone and her maid [here](https://www.encyclopedia-titanica.org/titanic-survivor/martha-evelyn-stone.html). There is also information about the port of embarkation in this article.  \n",
    "\n",
    "-----\n",
    "\n",
    "We can impute missing values also in the case of categorical variables that have values stored as strings. Let's impute the missing values of the 'Embarked' column in the `X_train` data. We set `strategy = constant` that allows us to specify the `fill_value` used to replace missing values. This can be used with strings or numeric data as well. The second option for strategy is `most_frequent` when the missing values will be replaced using the most frequent column value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SimpleImputer object and store it to variable imputer_cat\n",
    "imputer_cat = SimpleImputer(missing_values = np.nan, strategy = 'constant',fill_value = 'S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit column 'Embarked' in X_train\n",
    "imputer_cat.fit(X_train[['Embarked']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform column 'Embarked' in X_train\n",
    "X_train['Embarked'] = imputer_cat.transform(X_train[['Embarked']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the total number of missing values in column 'Embarked' to see whether these values have been replaced\n",
    "X_train['Embarked'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 4 >>>>> Repeat the imputing also for the column 'Embarked' in X_test data\n",
    "\n",
    "X_test['Embarked'] = imputer_cat.transform(X_test[['Embarked']])\n",
    "\n",
    "# Get the total number of missing values in column Embarked to see whether these values have been replaced\n",
    "X_test['Embarked'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 MAR: Missing At Random\n",
    "\n",
    "We can say that the data is missing at random if the probability of being missing is the same only within groups defined by the observed data. An example of this case is when we take a sample from a population. The probability to be included depends on some known property. For example, when placed on a soft surface, a weighing scale may produce more missing values than when placed on a hard surface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TASK\n",
    "\n",
    "In this task you will be using the Avocado dataset. You will impute numeric missing values in the column 'Small Bags' with the median value using `SimpleImputer`. The second task is to impute missing values in the column 'Region' with the most frequent string value of this column also using `SimpleImputer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset 'Data/avocado_missing.csv' and store it to variable avocado\n",
    "avocado = pd.read_csv('../Data/avocado_missing.csv')\n",
    "# Print the first 5 rows\n",
    "avocado.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK >>>> Print the total number of missing values\n",
    "\n",
    "avocado.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK >>>> Create SimpleImputer object and store it in variable imputer_median\n",
    "# Specify that you want to impute the median value\n",
    "\n",
    "imputer_median = SimpleImputer(missing_values = np.nan, strategy = 'median')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK >>>> Fit column 'Small Bags' using imputer_median \n",
    "\n",
    "imputer_median.fit(avocado[['Small Bags']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the median value that will be used to replace the missing values\n",
    "imputer_median.statistics_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK >>>> Transform the column 'Small Bags' using imputer_median\n",
    "# Assign the transformation to avocado['Small Bags']\n",
    "\n",
    "avocado['Small Bags'] = imputer_median.transform(avocado[['Small Bags']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK >>>> Create a SimpleImputer object and store it in the variable imputer_freq\n",
    "# Specify that you want to impute the most frequent value\n",
    "\n",
    "imputer_freq = SimpleImputer(missing_values = np.nan, strategy = 'most_frequent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK >>>> Fit the column 'region' using imputer_freq\n",
    "\n",
    "imputer_freq.fit(avocado[['region']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the most frequent value that will be used to replacing missing values\n",
    "imputer_freq.statistics_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK >>>> Transform column 'region' using imputer_freq\n",
    "# Assign the transformation to avocado['region']\n",
    "\n",
    "avocado['region'] = imputer_freq.transform(avocado[['region']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the total number of missing values to see whether the missing values have been replaced\n",
    "avocado.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Read only - Column Transformer\n",
    "\n",
    "Commonly, preparing data for machine learning models often involves several transformations such as imputing missing values, scaling numerical values, or encoding categorical features applied for particular columns. `scikit learn` offers the [`ColumnTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html) class that is used to apply different transformers to columns. This column transformer can be chained with Pipelines along with machine learning model. You can read more about `ColumnTransformer` [here](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix\n",
    "\n",
    "$^{1}$ Inference and missing data, DONALD B. RUBIN, Biometrika, Volume 63, Issue 3, December 1976, Pages 581â€“592\n",
    "\n",
    "Data source: \n",
    "\n",
    "Titanic dataset: https://www.kaggle.com/hesh97/titanicdataset-traincsv\n",
    "\n",
    "Data license: CC0: Public Domain\n",
    "\n",
    "Avocado dataset: https://www.kaggle.com/neuromusic/avocado-prices\n",
    "\n",
    "Data license: Database: Open Database\n",
    "\n",
    "Material adapted for RBI internal purposes with full permissions from original authors. [Source](https://github.com/zatkopatrik/authentic-data-science)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
